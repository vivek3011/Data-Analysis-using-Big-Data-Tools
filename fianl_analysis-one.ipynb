{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba94c79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T07:37:17.469378Z",
     "iopub.status.busy": "2022-09-15T07:37:17.468380Z",
     "iopub.status.idle": "2022-09-15T07:49:07.467935Z",
     "shell.execute_reply": "2022-09-15T07:49:07.448004Z",
     "shell.execute_reply.started": "2022-09-15T07:37:17.469378Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "E:\\Anaconda\\envs\\py3env\\lib\\site-packages\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.64 s\n",
      "Wall time: 11min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# connection establishment\n",
    "conn = psycopg2.connect(\n",
    "database=\"postgres\",\n",
    "\tuser='postgres',\n",
    "\tpassword='vivek@3011',\n",
    "\thost='localhost',\n",
    "\tport= '5432'\n",
    ")\n",
    "\n",
    "conn.autocommit = True\n",
    "\n",
    "# Creating a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# query to create a database\n",
    "sql = ''' CREATE DATABASE yellow_tripdata_analysis; ''';\n",
    "\n",
    "# executing above query\n",
    "cursor.execute(sql)\n",
    "#print(\"Database has been created successfully !!\");\n",
    "\n",
    "# Closing the connection\n",
    "conn.commit()\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.driver.extraClassPath', 'E:/IITI/data_analytics/crio_project/big_data/data_bricks/New folder/postgresql-42.3.7.jar'), \\\n",
    "                                   ('spark.executor.extraClassPath', 'E:/IITI/data_analytics/crio_project/big_data/data_bricks/New folder/postgresql-42.3.7.jar'),\\\n",
    "                                   (\"spark.jars\", \"E:/IITI/data_analytics/crio_project/big_data/data_bricks/New folder/postgresql-42.3.7.jar\"),\\\n",
    "                                   (\"spark.executor.extraLibraryPath\",\"E:/IITI/data_analytics/crio_project/big_data/data_bricks/New folder/postgresql-42.3.7.jar\"),\\\n",
    "                                    ])\n",
    "\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "def get_mode(df):\n",
    "    column_lst = df.columns\n",
    "    res = [df.select(i).groupby(i).count().orderBy(\"count\", ascending=False) for i in column_lst]\n",
    "    df_mode = res[0].limit(1).select(column_lst[0]).withColumn(\"temp_name_monotonically_increasing_id\", monotonically_increasing_id())\n",
    "    \n",
    "    for i in range(1, len(res)):\n",
    "        df2 = res[i].limit(1).select(column_lst[i]).withColumn(\"temp_name_monotonically_increasing_id\", monotonically_increasing_id())\n",
    "        df_mode = df_mode.join(df2, (df_mode.temp_name_monotonically_increasing_id == df2.temp_name_monotonically_increasing_id)).drop(df2.temp_name_monotonically_increasing_id)\n",
    "        \n",
    "    return df_mode.drop(\"temp_name_monotonically_increasing_id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf) \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName('yellow_tripdata_analysis') \\\n",
    "                    .getOrCreate()\n",
    "parquets=[\"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-01.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-02.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-03.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-04.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-05.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-06.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-07.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-08.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-09.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-10.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-11.parquet\",\n",
    "          \"data_parquet\\yellow_tripdata_2019\\yellow_tripdata_2019-12.parquet\"]\n",
    "\n",
    "data=[]\n",
    "for i in parquets:\n",
    "    d = spark.read.option(\"header\",\"true\").parquet(i,inferSchema=True)\n",
    "    data.append(d)\n",
    "    \n",
    "#Merge all the dataframes in list\n",
    "df_complete=unionAll(*data)\n",
    "#print(\"Total Rows and Columns:\",(df_complete.count(), len(df_complete.columns)))\n",
    "columns=df_complete.columns\n",
    "\n",
    "#Mode values for the DataFrame\n",
    "mode_values=get_mode(df_complete)\n",
    "\n",
    "\n",
    "pd=ps.DataFrame(df_complete)\n",
    "df_corr=pd.to_spark()\n",
    "df_corr=df_corr.withColumn(\"date_format\",to_date(df_complete.tpep_pickup_datetime,\"MM-dd-yyyy\"))\n",
    "df_corr=df_corr.withColumn(\"year\",year(df_corr.date_format))\n",
    "a=df_corr.first()[\"year\"]\n",
    "b=str(a)\n",
    "df_corr=df_corr.where(df_corr.date_format.contains(b))\n",
    "# df_corr=df_corr.where(df_corr.date_format.contains(\"2019\"))\n",
    "\n",
    "mode=[]\n",
    "for i in range(len(df_complete.columns)): \n",
    "    mode.append(mode_values.collect()[0][i])\n",
    "check=zip(df_complete.columns,mode)\n",
    "mydict=dict(check)\n",
    "a_dict = {key: mydict[key] for key in mydict if (key != 'VendorID' and key !='tpep_pickup_datetime' and key !='tpep_dropoff_datetime')} \n",
    "if a_dict[\"airport_fee\"]==None:\n",
    "          a_dict[\"airport_fee\"]=\"Unknown\"\n",
    "df_corr=df_corr.fillna(a_dict)\n",
    "\n",
    "\n",
    "\n",
    "#I made a little helper function for this that might help some people out.\n",
    "\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def rename_cols(agg_df, ignore_first_n=1):\n",
    "    \"\"\"changes the default spark aggregate names `avg(colname)` \n",
    "    to something a bit more useful. Pass an aggregated dataframe\n",
    "    and the number of aggregation columns to ignore.\n",
    "    \"\"\"\n",
    "    delimiters = \"(\", \")\"\n",
    "    split_pattern = '|'.join(map(re.escape, delimiters))\n",
    "    splitter = partial(re.split, split_pattern)\n",
    "    split_agg = lambda x: '_'.join(splitter(x))[0:-ignore_first_n]\n",
    "    renamed = map(split_agg, agg_df.columns[ignore_first_n:])\n",
    "    renamed = zip(agg_df.columns[ignore_first_n:], renamed)\n",
    "    for old, new in renamed:\n",
    "        agg_df = agg_df.withColumnRenamed(old, new)\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "\n",
    "week_level=df_corr.drop('RatecodeID',\n",
    " 'store_and_fwd_flag',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',)\n",
    "week_level=week_level.withColumn(\"week_date\",date_trunc(\"week\",week_level.date_format))\n",
    "column_change_week_level=week_level.groupBy(\"VendorID\",\"week_date\").sum()\n",
    "final1=rename_cols(column_change_week_level)\n",
    "final_week_level=final1.drop(\"sum_VendorID\",\"sum_payment_type\",\"sum_passenger_count\")\n",
    "\n",
    "\n",
    "\n",
    "month_level=df_corr.drop('RatecodeID',\n",
    " 'store_and_fwd_flag',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',)\n",
    "month_level=month_level.withColumn(\"Month\",month(month_level.date_format))\n",
    "column_change_month_level=month_level.groupBy(\"VendorID\",\"Month\").sum()\n",
    "final2=rename_cols(column_change_month_level)\n",
    "final_month_level=final2.drop(\"sum_VendorID\",\"sum_payment_type\",\"sum_Month\",\"sum_passenger_count\")\n",
    "\n",
    "\n",
    "avg_month=df_corr.drop('RatecodeID',\n",
    " 'store_and_fwd_flag',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',)\n",
    "avg_month=avg_month.withColumn(\"Month\",month(month_level.date_format))\n",
    "final_avg_month=avg_month.groupby(\"VendorID\",\"Month\").agg({\"congestion_surcharge\":\"avg\"}).alias('Avg_congestion_surcharge').orderBy(\"Month\", ascending=False)\n",
    "  \n",
    "\n",
    "\n",
    "pass_month=df_corr.drop('RatecodeID',\n",
    " 'store_and_fwd_flag',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',)\n",
    "pass_month=pass_month.withColumn(\"Month\",month(month_level.date_format))\n",
    "final_pass_month=pass_month.groupby(\"VendorID\",\"Month\").agg({\"passenger_count\":\"sum\"}).alias('Total_passenger_count').orderBy(\"Month\")   \n",
    "\n",
    "\n",
    "trip_hour=df_corr.drop('RatecodeID',\n",
    " 'store_and_fwd_flag',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',)\n",
    "trip_hour=trip_hour.withColumn(\"hour\", hour(trip_hour.tpep_pickup_datetime))\n",
    "trip1=trip_hour.groupby(\"hour\").agg(count(\"tpep_pickup_datetime\").alias('total_trip_count')).orderBy(\"hour\")\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "trip1 = trip1.withColumn('percent', f.col('total_trip_count')/f.sum('total_trip_count').over(Window.partitionBy()))\n",
    "final_trip_hour=trip1.orderBy('percent', ascending=False)\n",
    "\n",
    "\n",
    "payment_month=df_corr.drop('RatecodeID',\n",
    " 'store_and_fwd_flag',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',)\n",
    "payment_month=payment_month.withColumn(\"Month\",month(month_level.date_format))\n",
    "from pyspark.sql.functions import when\n",
    "payment_month = payment_month.withColumn(\"payment_type_name\", \n",
    "              when(payment_month.payment_type == 0, \"Credit card\")\n",
    "             .when(payment_month.payment_type == 1, \"Cash\")\n",
    "             .when(payment_month.payment_type == 2, \"No charge\")                       \n",
    "             .when(payment_month.payment_type == 3, \"Dispute\")  \n",
    "             .when(payment_month.payment_type == 4, \"Unknown\")  \n",
    "             .when(payment_month.payment_type == 5, \"Voided trip\")  \n",
    "             .otherwise(payment_month.payment_type))\n",
    "payment_month1=payment_month.groupby(\"Month\",\"payment_type_name\").agg(count(\"payment_type_name\").alias('payment_type_name_count')).orderBy(\"Month\")\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "payment_month1 = payment_month1.withColumn('percent', f.col('payment_type_name_count')/f.sum('payment_type_name_count').over(Window.partitionBy()))\n",
    "final_payment_month=payment_month1.orderBy('percent', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext \n",
    "#sc = SparkContext(\"local\", \"App Name\")\n",
    "#sql = SQLContext(sc)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "payment_month_top=payment_month.groupby(\"Month\",\"payment_type_name\").agg(count(\"payment_type_name\").alias('payment_type_name_count')).orderBy(\"Month\")\n",
    "window = Window.partitionBy(payment_month_top['Month']).orderBy(payment_month_top['payment_type_name_count'].desc())\n",
    "final_payment_month_top=payment_month_top.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 3 ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_week_level.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_analysis\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"Weekly_Sales_Aggr\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()\n",
    "\n",
    "final_month_level.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_analysis\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"Monthly_Sales_Aggr\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()\n",
    "\n",
    "final_avg_month.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_analysis\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"Avg_monthly_surcharge\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()\n",
    "\n",
    "final_pass_month.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_analysis\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"Total_Passengers_Monthly\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()\n",
    "\n",
    "final_trip_hour.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_analysis\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"Trips_Hourly\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()\n",
    "\n",
    "final_payment_month.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_analysis\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"Payment_Dist_Monthly\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()\n",
    "\n",
    "final_payment_month_top.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_analysis\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"Top_Payment_Monthly\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b2b963-a2dd-4e1e-a13c-e8b7fdd708a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T05:27:01.792745Z",
     "iopub.status.busy": "2022-09-15T05:27:01.792745Z",
     "iopub.status.idle": "2022-09-15T05:27:06.992950Z",
     "shell.execute_reply": "2022-09-15T05:27:06.990944Z",
     "shell.execute_reply.started": "2022-09-15T05:27:01.792745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database has been created successfully !!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# connection establishment\n",
    "conn = psycopg2.connect(\n",
    "database=\"postgres\",\n",
    "\tuser='postgres',\n",
    "\tpassword='vivek@3011',\n",
    "\thost='localhost',\n",
    "\tport= '5432'\n",
    ")\n",
    "\n",
    "conn.autocommit = True\n",
    "\n",
    "# Creating a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# query to create a database\n",
    "sql = ''' CREATE DATABASE final; ''';\n",
    "\n",
    "# executing above query\n",
    "cursor.execute(sql)\n",
    "print(\"Database has been created successfully !!\");\n",
    "\n",
    "# Closing the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d202596-e387-470f-87ce-89eba0cc6585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/supermart-DB\") \\\n",
    "    .option(\"dbtable\", \"customer\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"vivek@3011\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0189511-76a6-415d-b373-393eef2818c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------+---+-------------+---------------+--------------+-----------+-------+\n",
      "|customer_id|     customer_name|    segment|age|      country|           city|         state|postal_code| region|\n",
      "+-----------+------------------+-----------+---+-------------+---------------+--------------+-----------+-------+\n",
      "|   CG-12520|       Claire Gute|   Consumer| 67|United States|      Henderson|      Kentucky|      42420|  South|\n",
      "|   DV-13045|   Darrin Van Huff|  Corporate| 31|United States|    Los Angeles|    California|      90036|   West|\n",
      "|   SO-20335|    Sean O'Donnell|   Consumer| 65|United States|Fort Lauderdale|       Florida|      33311|  South|\n",
      "|   BH-11710|   Brosina Hoffman|   Consumer| 20|United States|    Los Angeles|    California|      90032|   West|\n",
      "|   AA-10480|      Andrew Allen|   Consumer| 50|United States|        Concord|North Carolina|      28027|  South|\n",
      "|   IM-15070|      Irene Maddox|   Consumer| 66|United States|        Seattle|    Washington|      98103|   West|\n",
      "|   HP-14815|     Harold Pawlan|Home Office| 20|United States|     Fort Worth|         Texas|      76106|Central|\n",
      "|   PK-19075|         Pete Kriz|   Consumer| 46|United States|        Madison|     Wisconsin|      53711|Central|\n",
      "|   AG-10270|   Alejandro Grove|   Consumer| 18|United States|    West Jordan|          Utah|      84084|   West|\n",
      "|   ZD-21925|Zuschuss Donatelli|   Consumer| 66|United States|  San Francisco|    California|      94109|   West|\n",
      "|   KB-16585|         Ken Black|  Corporate| 67|United States|        Fremont|      Nebraska|      68025|Central|\n",
      "|   SF-20065|   Sandra Flanagan|   Consumer| 41|United States|   Philadelphia|  Pennsylvania|      19140|   East|\n",
      "|   EB-13870|       Emily Burns|   Consumer| 34|United States|           Orem|          Utah|      84057|   West|\n",
      "|   EH-13945|     Eric Hoffmann|   Consumer| 21|United States|    Los Angeles|    California|      90049|   West|\n",
      "|   TB-21520|   Tracy Blumstein|   Consumer| 48|United States|   Philadelphia|  Pennsylvania|      19140|   East|\n",
      "|   MA-17560|      Matt Abelman|Home Office| 19|United States|        Houston|         Texas|      77095|Central|\n",
      "|   GH-14485|         Gene Hale|  Corporate| 28|United States|     Richardson|         Texas|      75080|Central|\n",
      "|   SN-20710|      Steve Nguyen|Home Office| 46|United States|        Houston|         Texas|      77041|Central|\n",
      "|   LC-16930|    Linda Cazamias|  Corporate| 31|United States|     Naperville|      Illinois|      60540|Central|\n",
      "|   RA-19885|      Ruben Ausman|  Corporate| 51|United States|    Los Angeles|    California|      90049|   West|\n",
      "+-----------+------------------+-----------+---+-------------+---------------+--------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20bf19-7040-4a85-ae5a-1fe70dc527df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71e4914-b7bd-4cba-a93f-6a28a178cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_2019\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"complete_data\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ccdf8-661a-4d0e-a2b2-f79bd2c306a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list=[final_week_level,final_month_level,final_avg_month,final_pass_month,final_trip,final_payment_month,fianl_payment_month_top]\n",
    "for i in final_list:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6db8edc-5e4b-4f56-a1c5-87e0c6636bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- segment: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postal_code: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"E:/IITI/data_analytics/cspark.jars.packagesrio_project/big_data/data_bricks/New folder/postgresql-42.3.7.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/supermart-DB\") \\\n",
    "    .option(\"dbtable\", \"customer\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"vivek@3011\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daba0957",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o60.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstudentDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmarks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://localhost:5432/yellow_tripdata_2019\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvivek@3011\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\py3env\\lib\\site-packages\\pyspark\\sql\\readwriter.py:966\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\py3env\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\py3env\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\py3env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o60.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "studentDf.select(\"id\",\"name\",\"marks\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/yellow_tripdata_2019\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"complete_data\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"vivek@3011\").save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
